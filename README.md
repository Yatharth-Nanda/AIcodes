# AIcodes
---

# Reinforcement Learning Algorithms and Temporal Difference Learning

This repository contains Python implementations of three common reinforcement learning algorithms - Epsilon-Greedy, Upper Confidence Bound (UCB), and Optimistic Initial Values. Additionally, it includes code for solving a Markov Reward Process using Temporal Difference (TD) Learning.

## Algorithms Implemented

### Epsilon-Greedy

- The Epsilon-Greedy algorithm is a simple exploration-exploitation strategy used in reinforcement learning.
- It balances between exploring new actions and exploiting the current best-known action.

### Upper Confidence Bound (UCB)

- The UCB algorithm is a more advanced exploration strategy that takes into account uncertainty in action values.
- It uses confidence bounds to make decisions on which actions to explore.

### Optimistic Initial Values

- The Optimistic Initial Values algorithm initializes action values with optimistic values.
- This encourages exploration early in the learning process.

## Solving Markov Reward Process (Temporal Difference Learning)

- Temporal Difference (TD) Learning is a method used to estimate the values of states in a Markov Reward Process.
- It combines Monte Carlo methods and dynamic programming to learn from each time step, making it efficient for online learning.

## Contributors

- List the contributors and their contributions here, if applicable.

## License

- This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- pseudo code as given in Reinforcement Learning :An Introduction - Barto Sutton 
